cat > README.md << 'EOF'
# AI HR Assistant - Flask POC with Custom Embeddings

This is a Proof-of-Concept (POC) for an AI-powered HR virtual assistant using:

- ‚úÖ Flask (Web UI)
- ‚úÖ LangChain (orchestration)
- ‚úÖ Custom Embedding Model: `renshanhf/weighted-triplet-finetuned-model`
- ‚úÖ FAISS (local vector search for POC)
- ‚úÖ Azure OpenAI GPT-35 (LLM)
- ‚úÖ Azure AI Content Safety (guardrails)

---

## üì¶ Features

- Fully containerized Flask web app
- Retrieval-Augmented Generation (RAG) pipeline
- Custom embeddings with your fine-tuned model
- Guardrails to ensure safe answers
- Runs locally or inside Docker
- Portable design for easy production upgrade

---

## üìÇ Project Structure

```bash
ai-hr-assistant-flask-poc/
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Flask Web App
‚îÇ   ‚îú‚îÄ‚îÄ langchain_logic.py   # RAG Pipeline Orchestration
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py        # Embedding + Vector Search Logic
‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ       ‚îî‚îÄ‚îÄ index.html       # Simple Web UI
‚îÇ
‚îú‚îÄ‚îÄ .env                     # Environment variables (secrets)
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îú‚îÄ‚îÄ Dockerfile               # Docker build
‚îú‚îÄ‚îÄ docker-compose.yml       # Local compose file
‚îî‚îÄ‚îÄ README.md                # This file
```

‚öôÔ∏è Prerequisites
Python 3.10+ (for local dev)

Docker (for containerization)

Azure Account:

Azure OpenAI resource

Azure Content Safety resource

HuggingFace model access (for embedding model)

üöÄ Quickstart
1Ô∏è‚É£ Clone Repository
bash
Copy
Edit
git clone <your-repo-url>
cd ai-hr-assistant-flask-poc
2Ô∏è‚É£ Setup Environment Variables
Create a .env file in the root directory:

bash
Copy
Edit
AZURE_OPENAI_ENDPOINT=https://<your-openai-endpoint>.openai.azure.com/
AZURE_OPENAI_KEY=<your-openai-key>
AZURE_OPENAI_DEPLOYMENT=gpt-35-turbo

AZURE_CONTENT_SAFETY_ENDPOINT=https://<your-contentsafety-endpoint>.cognitiveservices.azure.com/
AZURE_CONTENT_SAFETY_KEY=<your-contentsafety-key>
‚Ñπ Note: We are using FAISS locally for vector search in this POC, so Azure Cognitive Search is not required for now.

3Ô∏è‚É£ Build & Run Locally with Docker
Using plain Docker:
bash
Copy
Edit
docker build -t hr-virtual-assistant .
docker run -p 5000:5000 --env-file .env hr-virtual-assistant
OR use docker-compose:
bash
Copy
Edit
docker-compose up
4Ô∏è‚É£ Open Web Interface
Open your browser and navigate to:

arduino
Copy
Edit
http://localhost:5000
‚úÖ Start asking HR questions!

üß† How It Works
1Ô∏è‚É£ User enters HR question via web UI
2Ô∏è‚É£ The question is embedded using your custom embedding model
3Ô∏è‚É£ FAISS searches for top relevant documents locally
4Ô∏è‚É£ LangChain injects context + question to Azure OpenAI LLM
5Ô∏è‚É£ LLM generates response
6Ô∏è‚É£ Azure Content Safety checks the response for unsafe content
7Ô∏è‚É£ Answer displayed to user

üöÄ Next Steps
‚úÖ Add real HR documents into app/embeddings.py for better responses

‚úÖ Swap FAISS to Azure Cognitive Search for full enterprise deployment

‚úÖ Deploy Flask container to Azure Container Apps

‚úÖ Add CI/CD pipeline for automated deployment

üîí Security Notes
Make sure secrets in .env are not committed into your version control

In production, move secrets to Azure Key Vault or container secrets

üìû Support
Contact AI Platform Engineering for support, architecture review, or production upgrade.

